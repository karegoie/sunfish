# Transformer Model Configuration for Sunfish

[model]
# Model dimension - must be divisible by num_heads
d_model = 512

# Number of encoder and decoder layers
num_encoder_layers = 6
num_decoder_layers = 6

# Number of attention heads
num_heads = 8

# Feed-forward network dimension
d_ff = 2048

[training]
# Dropout rate for regularization
dropout_rate = 0.1

# Batch size
batch_size = 32

# Number of training epochs
num_epochs = 10

[adam]
learning_rate = 0.0001
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-08
weight_decay = 0.0

[parallel]
# Number of threads for parallel computation
num_threads = 4

[cwt]
# CWT scales for wavelet feature extraction
scales = [2.0, 4.0, 8.0, 16.0, 32.0]

[sliding_window]
# Sliding window configuration for training and prediction
window_size = 5000
window_overlap = 1000

[paths]
# Input/output file paths
train_fasta = "data/NC_001133.9.fasta"
train_gff = "data/NC_001133.9.gff"
predict_fasta = "data/NC_001133.9.fasta"
output_gff = "output.gff"
output_bedgraph = "output.bedgraph"
model_path = "sunfish.model"
