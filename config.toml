# Transformer Model Configuration for Sunfish

[model]
# Model dimension - must be divisible by num_heads
d_model = 64

# Number of encoder and decoder layers
num_encoder_layers = 4

# Number of attention heads
num_heads = 4

# Feed-forward network dimension
d_ff = 64

[training]
# Dropout rate for regularization
dropout_rate = 0.1

# Batch size
batch_size = 32

# Number of training epochs
num_epochs = 10

[adam]
learning_rate = 0.0001
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-08
weight_decay = 0.0

[parallel]
# Number of threads for parallel computation
num_threads = 96

[cwt]
# CWT scales for wavelet feature extraction
scales = [3.0, 4.0, 5.0, 10.0, 27.0, 81.0, 100.0]

[sliding_window]
# Sliding window configuration for training and prediction
window_size = 500
window_overlap = 100

[paths]
# Input/output file paths
train_fasta = "data/NC_001133.9.fasta"
train_gff = "data/NC_001133.9.gff"
predict_fasta = "data/NC_001133.9.fasta"
output_gff = "output.gff"
output_bedgraph = "output.bedgraph"
model_path = "sunfish.model"
