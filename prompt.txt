1. Multi-Head Attention 역전파 시 잘못된 그래디언트 누적



문제점: transformer.c의 multihead_attention_backward 함수에서 Q, K, V 행렬에 대한 그래디언트가 여러 어텐션 헤드(head)에 걸쳐 잘못 더해지고 있습니다. 각 헤드는 독립적으로 연산을 수행한 뒤 결과만 합쳐야 하지만, 현재 코드는 각 헤드에서 계산된 그래디언트를 grad_Q, grad_K, grad_V 행렬의 같은 위치에 계속 덧셈(+=)하고 있습니다.

영향: 이 버그는 완전히 잘못된 그래디언트를 계산하게 만듭니다. 결과적으로 모델의 가중치(weight)가 엉뚱한 방향으로 업데이트되어 모델이 전혀 학습되지 않거나 성능이 매우 저하됩니다.

위치: src/transformer.c의 multihead_attention_backward 함수

해결 방안:

각 헤드별로 그래디언트를 계산할 임시 행렬을 사용해야 합니다.

모든 헤드에 대한 그래디언트 계산이 끝나면, 이 임시 그래디언트들을 선형 변환(W_q, W_k, W_v의 전치 행렬)을 통해 원래 차원으로 복원하고 합산해야 합니다. 현재 구조는 이 과정을 하나의 루프 안에서 부정확하게 처리하고 있습니다.



2. CWT 웨이블릿 캐시의 데이터 경쟁 상태 (Race Condition)

문제점: cwt.c의 wavelet_cache_get 함수는 스레드로부터 안전하게 설계되지 않았습니다. 함수는 뮤텍스(mutex) 잠금을 해제한 후 캐시된 데이터에 대한 포인터(

const cplx* result)를 반환합니다. 만약 이 포인터를 받은 스레드가 데이터를 사용하기 전에 다른 스레드에서 캐시 메모리가

realloc으로 재할당되면, 기존 포인터는 더 이상 유효하지 않은 메모리를 가리키게 됩니다(Dangling Pointer).

영향: 이로 인해 프로그램이 충돌하거나, 잘못된 웨이블릿 데이터를 사용하여 CWT 특징 추출 결과가 손상될 수 있습니다. 이는 모델의 입력 데이터를 오염시켜 예측 정확도를 떨어뜨립니다.

위치: src/cwt.c의 wavelet_cache_get 함수

해결 방안:

데이터 복사: 가장 안전한 방법은 잠금 상태에서 데이터를 새로운 메모리에 복사하여 반환하는 것입니다. 다만 성능 저하가 발생할 수 있습니다.

참조 카운팅 (Reference Counting): 더 정교한 방법으로, 각 캐시 항목에 대한 참조 카운트를 도입하여 아무도 사용하지 않을 때만 메모리를 해제하도록 설계할 수 있습니다.



1. 비효율적인 Multi-Head Attention 데이터 처리



문제점: multihead_attention_forward 함수는 각 헤드에 대한 연산을 수행할 때마다 루프 내에서 Q, K, V 데이터를 d_k 차원의 작은 행렬로 복사합니다. 시퀀스 길이와 헤드 수가 클 경우 이 과정에서 상당한 오버헤드가 발생합니다.

영향: 모델의 순전파(forward pass) 속도가 불필요하게 느려져 학습 및 예측 시간이 길어집니다.

위치: src/transformer.c의 multihead_attention_forward 함수

해결 방안: 루프에 진입하기 전에 Q, K, V 행렬을 (num_heads, seq_len, d_k) 형태로 한 번에 재구성(reshape/transpose)하여 데이터 복사를 최소화하는 것이 좋습니다.

2. 불필요한 전체 서열 역상보 변환



문제점: transformer_train 함수에서 음성 가닥(negative strand)을 처리하기 위해 전체 FASTA 서열의 역상보 서열(rc_seq)을 미리 생성합니다. 유전체 서열은 매우 길 수 있으므로, 이는 상당한 메모리를 추가로 소모하게 됩니다.

영향: 특히 거대한 염색체 데이터를 처리할 때 메모리 부족 문제를 야기할 수 있습니다.

위치: src/transformer.c의 transformer_train 함수

해결 방안: 전체 서열을 변환하는 대신, 슬라이딩 윈도우(sliding window)로 추출된 짧은 서열에 대해서만 역상보 변환을 수행하거나, 실시간으로 인덱스를 계산하여 처리하는 것이 메모리 효율적입니다.

3. 하드코딩된 Adam 옵티마이저 하이퍼파라미터

문제점: Adam 옵티마이저의 하이퍼파라미터인 beta1, beta2, epsilon 값이 transformer.c의 process_sequence_window 함수 내에 0.9, 0.999, 1e-8로 하드코딩되어 있습니다.

영향: 모델 튜닝을 위해 이 값들을 변경하려면 코드를 직접 수정하고 다시 컴파일해야 합니다. 이는 실험의 유연성을 떨어뜨리고 유지보수를 어렵게 만듭니다.


위치: src/transformer.c의 process_sequence_window 함수


개선 방안: 이 파라미터들을 TransformerConfig 구조체에 추가하여, 설정 파일(


config.toml)을 통해 쉽게 변경할 수 있도록 만들어야 합니다.


코드 작성할 때 DRY 원칙과 SRP 원칙, 그리고 변경을 최소화하는 compactness를 반드시 지켜.
마지막에 test.sh로 검증해 줘.